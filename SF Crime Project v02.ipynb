{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import appropriate libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Set Numpy to print all lines of arrays\n",
    "np.set_printoptions(threshold='nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into Pandas dataframe for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y' 'DateTime' 'Year' 'Month' 'Day' 'Hour' 'SecondsDelta'\n",
      " 'Weekend' 2003L 2004L 2005L 2006L 2007L 2008L 2009L 2010L 2011L 2012L\n",
      " 2013L 2014L 2015L 'Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep'\n",
      " 'Oct' 'Nov' 'Dec' 1L 2L 3L 4L 5L 6L 7L 8L 9L 10L 11L 12L 13L 14L 15L 16L\n",
      " 17L 18L 19L 20L 21L 22L 23L 24L 25L 26L 27L 28L 29L 30L 31L 'Friday'\n",
      " 'Monday' 'Saturday' 'Sunday' 'Thursday' 'Tuesday' 'Wednesday' '12AM' '1AM'\n",
      " '2AM' '3AM' '4AM' '5AM' '6AM' '7AM' '8AM' '9AM' '10AM' '11AM' '12PM' '1PM'\n",
      " '2PM' '3PM' '4PM' '5PM' '6PM' '7PM' '8PM' '9PM' '10PM' '11PM' 'BAYVIEW'\n",
      " 'CENTRAL' 'INGLESIDE' 'MISSION' 'NORTHERN' 'PARK' 'RICHMOND' 'SOUTHERN'\n",
      " 'TARAVAL' 'TENDERLOIN']\n"
     ]
    }
   ],
   "source": [
    "# Extract new features here because it's easier in Pandas than NumPy\n",
    "def time_features(data):\n",
    "    data['DateTime'] = pd.to_datetime(data['Dates'])\n",
    "    data['Year'] = pd.DatetimeIndex(data['DateTime']).year\n",
    "    data['Month'] = pd.DatetimeIndex(data['DateTime']).month\n",
    "    data['Day'] = pd.DatetimeIndex(data['DateTime']).day\n",
    "    data['Hour'] = pd.DatetimeIndex(data['DateTime']).hour\n",
    "    data['SecondsDelta'] = (data.DateTime - pd.Timestamp('2013-01-01')) / np.timedelta64(1,'s')\n",
    "    data['Weekend'] = (data.DayOfWeek == \"Saturday\") | (data.DayOfWeek == \"Sunday\")\n",
    "    years = pd.get_dummies(data.Year)\n",
    "    months = pd.get_dummies(data.Month)\n",
    "    months.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    days = pd.get_dummies(data.Day)\n",
    "    daysofweek = pd.get_dummies(data.DayOfWeek)\n",
    "    hours = pd.get_dummies(data.Hour)\n",
    "    hours.columns = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM',\n",
    "                     '6AM', '7AM', '8AM', '9AM', '10AM', '11AM',\n",
    "                     '12PM', '1PM', '2PM', '3PM', '4PM', '5PM',\n",
    "                     '6PM', '7PM', '8PM', '9PM', '10PM', '11PM']\n",
    "    districts = pd.get_dummies(data.PdDistrict)\n",
    "    new_data = pd.concat([data, years, months, days, daysofweek, hours, districts], axis=1)\n",
    "    return new_data\n",
    "\n",
    "data = time_features(data)\n",
    "test = time_features(test)\n",
    "\n",
    "print data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Convert Pandas dataframe into Numpy array\n",
    "# data_np = np.array(data.values)\n",
    "# test_np = np.array(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Scale features between minimum and maximum values\n",
    "# def scale_features(data):\n",
    "#     scaled_data = np.arange(data.shape[0])\n",
    "#     for col in range(1, data.shape[1]):\n",
    "#         if (type(data[0, col]) == int) | (type(data[0, col]) == float):\n",
    "#             mms = MinMaxScaler()\n",
    "#             new_col = mms.fit_transform(data[:, col])\n",
    "#             scaled_data = np.column_stack((scaled_data, new_col))\n",
    "#         else:\n",
    "#             scaled_data = np.column_stack((scaled_data, data[:, col]))\n",
    "#     return scaled_data\n",
    "\n",
    "# scale = False\n",
    "# if scale:\n",
    "#     data_np2 = scale_features(data_np)\n",
    "#     test_np2 = scale_features(test_np)\n",
    "# else:\n",
    "#     data_np2 = np.copy(data_np)\n",
    "#     test_np2 = np.copy(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate labels\n",
    "labels = data.Category\n",
    "\n",
    "# Drop Category, Descript and Resolution columns since we cannot use them to predict\n",
    "train_data = data.drop(['Category', 'Descript', 'Resolution'], axis=1)\n",
    "train_names = train_data.columns.values.tolist()\n",
    "test_names = test.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Train Data: (5000, 110)\n",
      "Mini Train Labels: (5000L,)\n",
      "Regular Train Data: (433991, 110)\n",
      "Regular Train Labels: (433991L,)\n",
      "Dev Data: (438991, 110)\n",
      "Dev Labels: (438991L,)\n",
      "Test Data: (884262, 111)\n",
      "Columns in use: ['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'DateTime', 'Year', 'Month', 'Day', 'Hour', 'SecondsDelta', 'Weekend', 2003L, 2004L, 2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L, 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to features.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data.shape[0]))\n",
    "train_data = train_data.reindex(shuffle)\n",
    "labels = labels.reindex(shuffle)\n",
    "\n",
    "# Remove records where Y == 90\n",
    "train_data_clean = train_data[train_data.Y != 90]\n",
    "labels_clean = labels[train_data.Y != 90]\n",
    "num_examples = train_data_clean.shape[0]\n",
    "\n",
    "# Split the feature and label sets into train and dev sets\n",
    "mini_train_data = train_data_clean[:5000]\n",
    "mini_train_labels = labels_clean[:5000]\n",
    "\n",
    "reg_train_data = train_data_clean[5000:num_examples/2]\n",
    "reg_train_labels = labels_clean[5000:num_examples/2]\n",
    "\n",
    "dev_data = train_data_clean[num_examples/2:]\n",
    "dev_labels = labels_clean[num_examples/2:]\n",
    "\n",
    "test_data = test.copy()\n",
    "\n",
    "print \"Mini Train Data:\", mini_train_data.shape\n",
    "print \"Mini Train Labels:\", mini_train_labels.shape\n",
    "print \"Regular Train Data:\", reg_train_data.shape\n",
    "print \"Regular Train Labels:\", reg_train_labels.shape\n",
    "print \"Dev Data:\", dev_data.shape\n",
    "print \"Dev Labels:\", dev_labels.shape\n",
    "print \"Test Data:\", test_data.shape\n",
    "print \"Columns in use:\", train_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_submission(preds):\n",
    "    labels = [\"Id\",\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "              \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "              \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "              \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "              \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "              \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]\n",
    "    head_str = ','.join(labels)\n",
    "\n",
    "    num_cats = len(labels)\n",
    "    \n",
    "    # Make a dummy row to append to\n",
    "    ids = np.arange(preds.shape[0])[np.newaxis].transpose()\n",
    "    \n",
    "    results = np.column_stack((ids, preds))\n",
    "\n",
    "    num_form = ['%6f'] * (num_cats - 1)\n",
    "    num_form.insert(0, '%d')\n",
    "    # Write results to csv\n",
    "    np.savetxt('sample.csv', results, fmt=num_form, delimiter=',', header=head_str, comments='')\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 97\n"
     ]
    }
   ],
   "source": [
    "features_to_use = [2003L, 2004L, 2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L, \n",
    "                   'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', \n",
    "                   1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, \n",
    "                   16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, \n",
    "                   'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', \n",
    "                   '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', \n",
    "                   '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', \n",
    "                   'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', \n",
    "                   'SOUTHERN', 'TARAVAL', 'TENDERLOIN'\n",
    "                   ]\n",
    "\n",
    "print \"Number of features:\", len(features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 2.0}\n",
      "0.22839183301\n",
      "[mean: 0.00046, std: 0.00000, params: {'alpha': 0.0}, mean: 0.22836, std: 0.00073, params: {'alpha': 0.0001}, mean: 0.22836, std: 0.00073, params: {'alpha': 0.001}, mean: 0.22837, std: 0.00073, params: {'alpha': 0.01}, mean: 0.22837, std: 0.00073, params: {'alpha': 0.1}, mean: 0.22838, std: 0.00072, params: {'alpha': 0.5}, mean: 0.22839, std: 0.00072, params: {'alpha': 1.0}, mean: 0.22839, std: 0.00072, params: {'alpha': 2.0}, mean: 0.22838, std: 0.00074, params: {'alpha': 10.0}]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the optimal alpha is for a Bernoulli NB model\n",
    "\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "\n",
    "clf = GridSearchCV(BernoulliNB(), [alphas], scoring='accuracy')\n",
    "clf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "\n",
    "print clf.best_params_\n",
    "print clf.best_score_\n",
    "print clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.22779282491\n",
      "Log Loss: 2.56555153093\n"
     ]
    }
   ],
   "source": [
    "# How well does the Bernoulli NB model do with the selected alpha?\n",
    "\n",
    "bnb = BernoulliNB(alpha=2.0)\n",
    "bnb.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "bnb_probs = bnb.predict_proba(test_data[features_to_use])\n",
    "print \"BernoulliNB accuracy:\", bnb.score(dev_data[features_to_use], dev_labels)\n",
    "print \"Log Loss:\", log_loss(dev_labels, bnb.predict_proba(dev_data[features_to_use])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.227177778132\n",
      "Log Loss: 2.56686108234\n"
     ]
    }
   ],
   "source": [
    "# Maybe bagging the Bernoulli NB model does us some good\n",
    "\n",
    "bnb_bag = BaggingClassifier(BernoulliNB(alpha=2.0), max_features=0.8)\n",
    "bnb_bag.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "bnb_bag_probs = bnb_bag.predict_proba(test_data[features_to_use])\n",
    "print \"BernoulliNB accuracy:\", bnb_bag.score(dev_data[features_to_use], dev_labels)\n",
    "print \"Log Loss:\", log_loss(dev_labels, bnb_bag.predict_proba(dev_data[features_to_use])) \n",
    "\n",
    "# No significant improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.187010277342\n",
      "Log Loss: 2.77445906464\n"
     ]
    }
   ],
   "source": [
    "# Perhaps doing some dimensionality reduction will help us\n",
    "\n",
    "pca = PCA(n_components = 70)\n",
    "pca_train_data = pca.fit_transform(reg_train_data[features_to_use])\n",
    "pca_dev_data = pca.transform(dev_data[features_to_use])\n",
    "pca_test_data = pca.transform(test_data[features_to_use])\n",
    "\n",
    "colors = [\"#1f77b4\",\"#2ca02c\",\"#7f7f7f\",\"#8c564b\",\"#9edae5\",\n",
    "          \"#17becf\",\"#98df8a\",\"#9467bd\",\"#aec7e8\",\"#bcbd22\",\n",
    "          \"#c5b0d5\",\"#c7c7c7\",\"#c49c94\",\"#d62728\",\"#dbdb8d\",\n",
    "          \"#e377c2\",\"#f7b6d2\",\"#ff7f0e\",\"#ff9896\",\"#ffbb78\"]\n",
    "\n",
    "#plt.scatter(pca_train_data[:, 0], pca_train_data[:, 1], c = colors, s = 50)\n",
    "\n",
    "bnb2 = BernoulliNB()\n",
    "bnb2.fit(pca_train_data, reg_train_labels)\n",
    "bnb2_probs = bnb2.predict_proba(pca_test_data)\n",
    "print \"BernoulliNB accuracy:\", bnb2.score(pca_dev_data, dev_labels)\n",
    "print \"Log Loss:\", log_loss(dev_labels, bnb2.predict_proba(pca_dev_data)) \n",
    "\n",
    "# It doesn't seem that using PCA is giving us any better results than just the straight NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy: 0.169503261781\n",
      "Log Loss: 25.9921985239\n"
     ]
    }
   ],
   "source": [
    "# Let's try a decision tree\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "dt_probs = dt.predict_proba(test_data[features_to_use])\n",
    "print \"Decision Tree accuracy:\", dt.score(dev_data[features_to_use], dev_labels)\n",
    "print \"Log Loss:\", log_loss(dev_labels, dt.predict_proba(dev_data[features_to_use])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: 0.178657658807\n",
      "Log Loss: 17.526350391\n"
     ]
    }
   ],
   "source": [
    "# Let's try a random forest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "rf_probs = rf.predict_proba(test_data[features_to_use])\n",
    "print \"Random Forest accuracy:\", rf.score(dev_data[features_to_use], dev_labels)\n",
    "print \"Log Loss:\", log_loss(dev_labels, rf.predict_proba(dev_data[features_to_use])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try and find the optimal C for logistic regression\n",
    "cs = [0.001,  0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10]\n",
    "#cs = np.arange(0.1, 1.0, 0.1)\n",
    "parameters = dict(C = cs)\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), parameters, scoring='accuracy')\n",
    "clf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "print \"Default value of C is 1.0\"\n",
    "print \"Optimal value of C is\", str(clf.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.226161811973\n",
      "Log Loss: 2.68228615201\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with optimized C\n",
    "\n",
    "lr = LogisticRegression(C=0.001)\n",
    "lr.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "lr_probs = lr.predict_proba(test_data[features_to_use])\n",
    "print \"Logistic Regression accuracy:\", lr.score(dev_data[features_to_use], dev_labels)\n",
    "print \"Log Loss:\", log_loss(dev_labels, lr.predict_proba(dev_data[features_to_use])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try to average the probabilities of the NB and LR classifiers\n",
    "\n",
    "#avg_probs = (bnb.predict_proba(dev_data[features_to_use]) + lr.predict_proba(dev_data[features_to_use])) / 2\n",
    "avg_probs = (bnb_probs + lr_probs) / 2\n",
    "#print \"Log Loss:\", log_loss(dev_labels, avg_probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(avg_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
